{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extracting Process\n",
    "\n",
    "#### 解决click element not find的问题: 用ActionChain, 翻页后再点击\n",
    "`\n",
    "ElementClickInterceptedException: Message: element click intercepted: Element <span aria-hidden=\"true\" class=\"icon--24-chevron-right-v2 navigation-button-icon__373c0__2Fl7a css-12anxc3\">...</span> is not clickable at point (222, 588). Other element would receive the click: <html xmlns:fb=\"http://www.facebook.com/2008/fbml\" class=\"js\" lang=\"en\">...</html>\n",
    "  (Session info: headless chrome=88.0.4324.150)\n",
    " `\n",
    "\n",
    "#### Text cleaning checkpoints\n",
    "- Check out for null/empty/missing values\n",
    "- Remove characters\n",
    "    - punctuaion\n",
    "    - `\\n`, `\\t`, `\\r`等\n",
    "    - special characters like `!@#$%^&*()`\n",
    "    - 有些文本有那种`\\a0`什么，估计是emoji吧\n",
    "    - numbers are usually consider useless\n",
    "    - Contraction thing 有缩写字典，应该还好。这个可以处理。\n",
    "- Lower case\n",
    "- Spaces: double space -> single space\n",
    "\n",
    "上面的基本预处理完之后，再建立`nltk`/`gensim`/`sklearn`的文本处理模型。\n",
    "- BoW\n",
    "- TF-IDF\n",
    "\n",
    "\\* 注：这些文本矩阵要结合餐厅category来训练。\n",
    "\n",
    "#### Other cleaning\n",
    "- Date and datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys    # type things in the search bar and see the results\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from lxml import etree\n",
    "\n",
    "PATH = '/Users/liqingran/Downloads/chromedriver'\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options   # 实现无可视化界面\n",
    "from selenium.webdriver import ChromeOptions            # 实现selenium规避检测\n",
    "\n",
    "# 实例化options并进行参数设定，实现无可视化界面\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "\n",
    "# 实现检测规避，这个不懂怎么加，再说吧\n",
    "option = ChromeOptions()\n",
    "option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "# driver = Chrome(options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_review(parser):\n",
    "    \"\"\"Retrive real-time reviews for each dynamic source page.\"\"\"\n",
    "    info = []\n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    dates = []\n",
    "    name = parser.xpath('//*[@id=\"wrap\"]/div[3]/yelp-react-root/div/div[2]/div[1]/div[1]/div/div/div[1]/h1/text()')\n",
    "    li_list = parser.xpath('//*[@id=\"wrap\"]/div[3]/yelp-react-root/div/div[3]/div/div/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/ul/li')\n",
    "    for li in li_list:\n",
    "        review = li.xpath('./div/div[3]/p/span/text()')\n",
    "        rating = li.xpath('./div/div[2]/div/div[1]/span/div/@aria-label')\n",
    "        date = li.xpath('./div/div[2]/div/div[2]/span/text()')    \n",
    "        if review:    # 因为如果这个review没放图片就在div[3]，如果放了图片就在div[4]\n",
    "            reviews.append(review[0])\n",
    "        else:\n",
    "            review = li.xpath('./div/div[4]/p/span/text()')\n",
    "            reviews.append(review[0])\n",
    "        ratings.append(rating[0])\n",
    "        dates.append(date[0])\n",
    "\n",
    "    for i in range(len(reviews)):\n",
    "        dict_ = {\n",
    "            \"Name\": name[0],\n",
    "            \"Rating\": ratings[i],\n",
    "            \"Date\": dates[i],\n",
    "            \"Review\": reviews[i]       \n",
    "        }\n",
    "        info.append(dict_)        \n",
    "    return info\n",
    "\n",
    "\n",
    "def get_max_page(parser):\n",
    "    \"\"\"Get the total page number for further click iteration.\"\"\"\n",
    "    num = parser.xpath('//*[@id=\"wrap\"]/div[3]/yelp-react-root/div/div[3]/div/div/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/div[4]/div[2]/span/text()')\n",
    "    num = int(num[0].split(' ')[-1])\n",
    "    return num\n",
    "\n",
    "\n",
    "def organize_info(info):\n",
    "    \"\"\"Since each time we return lists of dictionaries, \n",
    "       this function is for concatinating all dictionaries into one single list.\"\"\"\n",
    "    final_info = []\n",
    "    for i in info:\n",
    "        for j in i:\n",
    "            final_info.append(j)\n",
    "    return final_info\n",
    "\n",
    "\n",
    "def next_page_button(parser):\n",
    "    \"\"\"Find the element of the clicking next page button.\"\"\"\n",
    "    button_xpath = '//*[@id=\"wrap\"]/div[3]/yelp-react-root/div/div[3]/div/div/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/div[4]/div[1]/div/div'\n",
    "    page_div = parser.xpath(button_xpath)\n",
    "    click_point = len(page_div)\n",
    "    click_xpath = button_xpath + str([click_point]) + '/span/a/span'\n",
    "    return click_xpath\n",
    "\n",
    "\n",
    "def scrap_review(url):\n",
    "#     PATH = '/Users/liqingran/Downloads/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path = PATH, options = chrome_options)\n",
    "\n",
    "    res = driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    parser = etree.HTML(str(driver.page_source).replace('<br>', '\\n').replace('</br>', '\\n'))\n",
    "    total_page_num = get_max_page(parser)\n",
    "    \n",
    "    next_page_xpath = next_page_button(parser)\n",
    "\n",
    "    info = []\n",
    "    # 点击 total_page_num-1 次，因为最后一页有可能不够20个review，我不想要了\n",
    "    if total_page_num < 10:\n",
    "        for i in range(total_page_num-1):\n",
    "            dict_ = retrive_review(parser)\n",
    "            info.append(dict_)\n",
    "            print(\"->Page\", i+1, \"sucess!\")\n",
    "#             driver.find_element_by_xpath(next_page_xpath).click()      # 找到下一页的span标签并点击\n",
    "            element = driver.find_element_by_xpath(next_page_xpath)\n",
    "            webdriver.ActionChains(driver).move_to_element(element).click(element).perform()\n",
    "            time.sleep(5)                                             \n",
    "            parser = etree.HTML(str(driver.page_source).replace('<br>', '\\n').replace('</br>', '\\n'))\n",
    "    else:\n",
    "        for i in range(9):\n",
    "            dict_ = retrive_review(parser)\n",
    "            info.append(dict_)\n",
    "            print(\"->Page\", i+1, \"sucess!\")\n",
    "            element = driver.find_element_by_xpath(next_page_xpath)\n",
    "            webdriver.ActionChains(driver).move_to_element(element).click(element).perform()\n",
    "            time.sleep(5)\n",
    "            parser = etree.HTML(str(driver.page_source).replace('<br>', '\\n').replace('</br>', '\\n'))\n",
    "#             try:\n",
    "#                 driver.find_element_by_xpath(next_page_xpath).click() \n",
    "#                 time.sleep(10)  \n",
    "#                 parser = etree.HTML(str(driver.page_source).replace('<br>', '\\n').replace('</br>', '\\n'))  \n",
    "#             except:\n",
    "#                 print(\"Click Interception, break on page\", i+1)\n",
    "#             finally:\n",
    "#                 driver.quit()\n",
    "#                 return info\n",
    "  \n",
    "    driver.quit()\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "->Page 9 sucess!\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "url_test = 'https://www.yelp.com/biz/capt-loui-fort-lee-2'\n",
    "info = scrap_review(url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for more businesses al-together\n",
    "url_list = ['https://www.yelp.com/biz/gopchang-story-fort-lee-fort-lee',\n",
    "             'https://www.yelp.com/biz/soup-dumpling-plus-fort-lee',\n",
    "             'https://www.yelp.com/biz/soba-noodle-azuma-fort-lee-2',\n",
    "             'https://www.yelp.com/biz/sa-rit-gol-fort-lee-2',\n",
    "             'https://www.yelp.com/biz/lauren-s-chicken-burger-fort-lee',\n",
    "             'https://www.yelp.com/biz/gamja-tang-tang-fort-lee',\n",
    "             'https://www.yelp.com/biz/oiso-bbq-pit-fort-lee-2',\n",
    "             'https://www.yelp.com/biz/capt-loui-fort-lee-2',\n",
    "             'https://www.yelp.com/biz/martys-fort-lee',\n",
    "             'https://www.yelp.com/biz/sushi-kai-fort-lee',\n",
    "             'https://www.yelp.com/biz/wok-bar-fort-lee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Page 1 sucess!\n",
      "The 1 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "->Page 9 sucess!\n",
      "The 2 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "->Page 9 sucess!\n",
      "The 3 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "The 4 business scraping success!\n",
      "->Page 1 sucess!\n",
      "The 5 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "->Page 9 sucess!\n",
      "The 6 business scraping success!\n",
      "->Page 1 sucess!\n",
      "The 7 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "->Page 6 sucess!\n",
      "->Page 7 sucess!\n",
      "->Page 8 sucess!\n",
      "->Page 9 sucess!\n",
      "The 8 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "->Page 5 sucess!\n",
      "The 9 business scraping success!\n",
      "The 10 business scraping success!\n",
      "->Page 1 sucess!\n",
      "->Page 2 sucess!\n",
      "->Page 3 sucess!\n",
      "->Page 4 sucess!\n",
      "The 11 business scraping success!\n"
     ]
    }
   ],
   "source": [
    "total_info = []\n",
    "for i in range(len(url_list)):\n",
    "    info = scrap_review(url_list[i])\n",
    "    info = organize_info(info)\n",
    "    total_info.append(info)\n",
    "    print(\"The\", i+1, \"business scraping success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_info = organize_info(total_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>12/25/2020</td>\n",
       "      <td>It was a delightful experience overall. Ambian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>12/10/2020</td>\n",
       "      <td>Great customer service. Most importantly, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>1/23/2021</td>\n",
       "      <td>The restaurant does not take reservations so w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>1 star rating</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>Used to love this place. But I and my friends ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>4 star rating</td>\n",
       "      <td>1/22/2021</td>\n",
       "      <td>Finally got to check out the Fort Lee location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name         Rating        Date  \\\n",
       "0  Gopchang Story  Fort Lee  5 star rating  12/25/2020   \n",
       "1  Gopchang Story  Fort Lee  5 star rating  12/10/2020   \n",
       "2  Gopchang Story  Fort Lee  5 star rating   1/23/2021   \n",
       "3  Gopchang Story  Fort Lee  1 star rating    2/6/2021   \n",
       "4  Gopchang Story  Fort Lee  4 star rating   1/22/2021   \n",
       "\n",
       "                                              Review  \n",
       "0  It was a delightful experience overall. Ambian...  \n",
       "1  Great customer service. Most importantly, safe...  \n",
       "2  The restaurant does not take reservations so w...  \n",
       "3  Used to love this place. But I and my friends ...  \n",
       "4  Finally got to check out the Fort Lee location...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data = pd.DataFrame(total_info)\n",
    "mini_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.to_csv('mini_review_dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把同一个餐厅的所有review都并起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cap't Loui</td>\n",
       "      <td>I have dined here a few times during the pande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamja Tang Tang</td>\n",
       "      <td>PRE COVID REVIEW!!!\\n\\nAbsolutely love this pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>It was a delightful experience overall. Ambian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lauren’s Chicken Burger</td>\n",
       "      <td>I can not get over how delicious my supreme ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marty's</td>\n",
       "      <td>I came here with a friend during COVID times a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OISO BBQ PIT</td>\n",
       "      <td>This BBQ joint is legit. Stick with the staple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sa Rit Gol</td>\n",
       "      <td>Atmosphere: 5\\nDue to COVID, we weren't able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soba Noodle Azuma</td>\n",
       "      <td>Covid times:\\n\\nOrdered take out a couple time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Soup Dumpling Plus</td>\n",
       "      <td>I am so happy that I FINALLY have an amazing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wok Bar</td>\n",
       "      <td>I ordered directly through their website, and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name                                             Review\n",
       "0                Cap't Loui  I have dined here a few times during the pande...\n",
       "1           Gamja Tang Tang  PRE COVID REVIEW!!!\\n\\nAbsolutely love this pl...\n",
       "2  Gopchang Story  Fort Lee  It was a delightful experience overall. Ambian...\n",
       "3   Lauren’s Chicken Burger  I can not get over how delicious my supreme ch...\n",
       "4                   Marty's  I came here with a friend during COVID times a...\n",
       "5              OISO BBQ PIT  This BBQ joint is legit. Stick with the staple...\n",
       "6                Sa Rit Gol  Atmosphere: 5\\nDue to COVID, we weren't able t...\n",
       "7         Soba Noodle Azuma  Covid times:\\n\\nOrdered take out a couple time...\n",
       "8        Soup Dumpling Plus  I am so happy that I FINALLY have an amazing s...\n",
       "9                   Wok Bar  I ordered directly through their website, and ..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这样一家餐厅爬下来的所有review都在一个column里面了\n",
    "reviews_df = pd.DataFrame(mini_data.groupby('Name')['Review'].sum()).reset_index()\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews_df.Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check\n",
    "- '\\xa0' -> ' '\n",
    "- '\\n' -> ' '\n",
    "- 符号：&, -, \\$, :, (, ), /, \n",
    "- double space 换成 single space\n",
    "- remove number\n",
    "- 缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "add_stop = ['said', 'say', '...', 'like', 'cnn', 'ad', 'thi', 'ha', 'wa']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
    "\n",
    "# Punctuations\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "# Tokenize\n",
    "def casual_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction\n",
    "c_dict = {\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"I would\",\"i'd've\": \"I would have\",\"i'll\": \"I will\",\"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\"i've\": \"I have\",\"isn't\": \"is not\", \"it'd\": \"it had\", \"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\", \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\"where've\": \"where have\", \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# compile the contraction dictionary\n",
    "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
    "def expandContractions(text, c_re = c_re):\n",
    "    def replace(match):\n",
    "        return c_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texing processing\n",
    "stemmer = PorterStemmer()\n",
    "def process_text(text):\n",
    "    # Simple processing\n",
    "    text = text.replace('\\a0', ' ').replace('\\n', ' ').replace('.', ' ').replace(',', ' ').replace(\"'\", \" \")\n",
    "    # 1. tokenize\n",
    "    text = casual_tokenizer(text)\n",
    "    # 2. lower case\n",
    "    text = [word.lower() for word in text]\n",
    "    # 3. remove numbers/digits\n",
    "    text = [re.sub(r'[0-9]+', '', word) for word in text]\n",
    "    # 4. replace contractions\n",
    "    text = [expandContractions(word, c_re) for word in text]\n",
    "    # 5. stemming\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    # 6. remove punctuations\n",
    "    text = [word for word in text if word not in punc]\n",
    "    # 7. remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    # 8. remove single letter \n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    # 9. clear spaces\n",
    "    text = [each for each in text if ' ' not in each]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews = reviews.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cap't Loui</td>\n",
       "      <td>I have dined here a few times during the pande...</td>\n",
       "      <td>[dine, time, dure, pandem, taken, food, dine, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamja Tang Tang</td>\n",
       "      <td>PRE COVID REVIEW!!!\\n\\nAbsolutely love this pl...</td>\n",
       "      <td>[pre, covid, review, absolut, love, place, esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gopchang Story  Fort Lee</td>\n",
       "      <td>It was a delightful experience overall. Ambian...</td>\n",
       "      <td>[delight, experi, overal, ambianc, nice, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lauren’s Chicken Burger</td>\n",
       "      <td>I can not get over how delicious my supreme ch...</td>\n",
       "      <td>[delici, suprem, chicken, burger, lauren, chic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marty's</td>\n",
       "      <td>I came here with a friend during COVID times a...</td>\n",
       "      <td>[came, friend, dure, covid, time, impress, saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OISO BBQ PIT</td>\n",
       "      <td>This BBQ joint is legit. Stick with the staple...</td>\n",
       "      <td>[bbq, joint, legit, stick, stapl, brisket, rib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sa Rit Gol</td>\n",
       "      <td>Atmosphere: 5\\nDue to COVID, we weren't able t...</td>\n",
       "      <td>[atmospher, covid, weren, abl, eat, order, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soba Noodle Azuma</td>\n",
       "      <td>Covid times:\\n\\nOrdered take out a couple time...</td>\n",
       "      <td>[covid, time, order, coupl, time, did, disappo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Soup Dumpling Plus</td>\n",
       "      <td>I am so happy that I FINALLY have an amazing s...</td>\n",
       "      <td>[happi, final, amaz, spot, chines, food, subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wok Bar</td>\n",
       "      <td>I ordered directly through their website, and ...</td>\n",
       "      <td>[order, directli, websit, deliveri, came, quic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  \\\n",
       "0                Cap't Loui   \n",
       "1           Gamja Tang Tang   \n",
       "2  Gopchang Story  Fort Lee   \n",
       "3   Lauren’s Chicken Burger   \n",
       "4                   Marty's   \n",
       "5              OISO BBQ PIT   \n",
       "6                Sa Rit Gol   \n",
       "7         Soba Noodle Azuma   \n",
       "8        Soup Dumpling Plus   \n",
       "9                   Wok Bar   \n",
       "\n",
       "                                              Review  \\\n",
       "0  I have dined here a few times during the pande...   \n",
       "1  PRE COVID REVIEW!!!\\n\\nAbsolutely love this pl...   \n",
       "2  It was a delightful experience overall. Ambian...   \n",
       "3  I can not get over how delicious my supreme ch...   \n",
       "4  I came here with a friend during COVID times a...   \n",
       "5  This BBQ joint is legit. Stick with the staple...   \n",
       "6  Atmosphere: 5\\nDue to COVID, we weren't able t...   \n",
       "7  Covid times:\\n\\nOrdered take out a couple time...   \n",
       "8  I am so happy that I FINALLY have an amazing s...   \n",
       "9  I ordered directly through their website, and ...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  [dine, time, dure, pandem, taken, food, dine, ...  \n",
       "1  [pre, covid, review, absolut, love, place, esp...  \n",
       "2  [delight, experi, overal, ambianc, nice, great...  \n",
       "3  [delici, suprem, chicken, burger, lauren, chic...  \n",
       "4  [came, friend, dure, covid, time, impress, saf...  \n",
       "5  [bbq, joint, legit, stick, stapl, brisket, rib...  \n",
       "6  [atmospher, covid, weren, abl, eat, order, pro...  \n",
       "7  [covid, time, order, coupl, time, did, disappo...  \n",
       "8  [happi, final, amaz, spot, chines, food, subje...  \n",
       "9  [order, directli, websit, deliveri, came, quic...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['clean_review'] = clean_reviews\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv('clean_review_table.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
